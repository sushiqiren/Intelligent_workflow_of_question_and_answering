!pip install transformers
!pip install transformers[torch] --upgrade
!pip install accelerate --upgrade
!pip install datasets
!pip install evaluate

from google.colab import files
uploaded = files.upload()

from datasets import Dataset, DatasetDict, Features, Value, Sequence, Array2D
import datasets
import json
import pandas as pd


with open('dataset.json', 'r') as json_file:
    squad_data = json.load(json_file)
# Create an empty list to store the data
data_list = []

# Iterate through the JSON data to structure it as desired
for article in squad_data['data']:

    for paragraph in article['paragraphs']:
        context = paragraph["context"]
        for qa in paragraph["qas"]:
            question = qa["question"]
            id_ = qa["id"]
            answer_starts = [answer["answer_start"] for answer in qa["answers"]]
            answers = [answer["text"] for answer in qa["answers"]]

            data_list.append({
                'id': id_,
                'context': context,
                'question': question,
                'answers': {
                    'answer_start': answer_starts,
                    'text': answers,
                }
            })

# Define the custom features
features=datasets.Features(
                {
                    "id": datasets.Value("string"),
                    "context": datasets.Value("string"),
                    "question": datasets.Value("string"),
                    "answers": datasets.features.Sequence(
                        {
                            "text": datasets.Value("string"),
                            "answer_start": datasets.Value("int32"),
                        }
                    ),
                }
            )

# Create a Pandas DataFrame from the structured data
df = pd.DataFrame(data_list)

# Access the 'answers' column of the first row using iloc
sample_answers = df['answers'].iloc[0]

# Print the structure of the 'answers' column for the first row
print(sample_answers)

# Create a Dataset from the Pandas DataFrame with explicit features
dataset = Dataset.from_pandas(df, features=features)

# Split the dataset into training and validation sets if needed by selecting specific ranges
train_dataset = dataset.select(range(739))  # Adjust the range based on your desired split
valid_dataset = dataset.select(range(739, len(dataset)))

# Create a DatasetDict
raw_datasets = DatasetDict({'train': train_dataset, 'validation': valid_dataset})
